{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve, auc\n",
    "from scipy.stats import sem\n",
    "from capymoa.anomaly import HalfSpaceTrees, Autoencoder, OnlineIsolationForest, StreamRHF\n",
    "from capymoa.datasets import Electricity, Hyper100k, CovtypeNorm, RTG_2abrupt\n",
    "from capymoa.evaluation import AnomalyDetectionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets and models\n",
    "#datasets = [Electricity, Hyper100k, CovtypeNorm, RTG_2abrupt]\n",
    "#datasets = [Electricity, Hyper100k]\n",
    "datasets = [Hyper100k]\n",
    "models = {\n",
    "#    \"HalfSpaceTrees\": HalfSpaceTrees,\n",
    "#    \"Autoencoder\": Autoencoder,\n",
    "#    \"OnlineIsolationForest\": OnlineIsolationForest,\n",
    "    \"StreamRHF\": StreamRHF\n",
    "}\n",
    "\n",
    "# Directory for saving results\n",
    "results_dir = os.path.join(os.getcwd(), \"results\")\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Results storage\n",
    "all_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Hyper100k\n",
      "\n",
      "Running Model: StreamRHF\n",
      "Run 1\n",
      "our StreamRHF initialized\n",
      "20\n",
      "Checkpoint saved for model StreamRHF at /home/infres/cchavez-23/Stream-Random-Histogram-Forest/src/capymoa/anomaly/tests capymoa datasets/results/streamrhf_hyper100k_results_checkpoint.csv\n",
      "Summary saved for StreamRHF at /home/infres/cchavez-23/Stream-Random-Histogram-Forest/src/capymoa/anomaly/tests capymoa datasets/results/StreamRHF/Hyper100k_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2394272/3658170013.py:110: SmallSampleWarning: One or more sample arguments is too small; all returned values will be NaN. See documentation for sample size requirements.\n",
      "  summary = {metric: (np.mean(scores), sem(scores) * 1.96) for metric, scores in metrics.items()}\n"
     ]
    }
   ],
   "source": [
    "# Main loop over datasets and models\n",
    "for dataset_cls in datasets:\n",
    "    dataset_name = dataset_cls.__name__\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    stream = dataset_cls()\n",
    "    schema = stream.get_schema()\n",
    "\n",
    "    # Load dataset labels\n",
    "    labels = []\n",
    "\n",
    "    processed_instances = 0\n",
    "    #while processed_instances < 10:\n",
    "    #    processed_instances += 1\n",
    "\n",
    "    while processed_instances < 20000 and stream.has_more_instances():\n",
    "        instance = stream.next_instance()\n",
    "        labels.append(instance.y_index)\n",
    "        processed_instances += 1\n",
    "    \n",
    "    #while stream.has_more_instances():\n",
    "    #    instance = stream.next_instance()\n",
    "    #    labels.append(instance.y_index)\n",
    "    processed_instances = 0 \n",
    "    labels = np.array(labels)\n",
    "    stream.restart()\n",
    "\n",
    "    for model_name, ModelClass in models.items():\n",
    "        print(f\"\\nRunning Model: {model_name}\")\n",
    "        model_results = []\n",
    "        ap_scores, auc_scores, auc_paper_scores, auc_capymoa_scores, execution_times = [], [], [], [], []\n",
    "\n",
    "        for run in range(1):  #For now, let's just try one run per dataset and model since they are big\n",
    "            print(f\"Run {run + 1}\")\n",
    "            learner = ModelClass(schema)\n",
    "            evaluator = AnomalyDetectionEvaluator(schema)\n",
    "\n",
    "            stream.restart()\n",
    "            anomaly_scores = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            #processed_instances = 0\n",
    "            #while processed_instances < 10:\n",
    "            #    processed_instances += 1\n",
    "            #while stream.has_more_instances():\n",
    "            #    instance = stream.next_instance()\n",
    "            #    proba = learner.score_instance(instance)\n",
    "            #    anomaly_scores.append(1 - proba)  # Flip score for consistency\n",
    "            #    evaluator.update(instance.y_index, proba)\n",
    "            #    learner.train(instance)\n",
    "\n",
    "            processed_instances = 0\n",
    "\n",
    "            while processed_instances < 20000 and stream.has_more_instances():\n",
    "                instance = stream.next_instance()\n",
    "                proba = learner.score_instance(instance)\n",
    "                anomaly_scores.append(1 - proba)  # Flip score for consistency\n",
    "                evaluator.update(instance.y_index, proba)\n",
    "                learner.train(instance)\n",
    "                processed_instances += 1\n",
    "\n",
    "            # Compute metrics\n",
    "            auc_score_capymoa = evaluator.auc()\n",
    "            anomaly_scores = np.array(anomaly_scores)\n",
    "            ap_score = average_precision_score(labels, anomaly_scores)\n",
    "            auc_score = roc_auc_score(labels, anomaly_scores)\n",
    "            fpr, tpr, _ = roc_curve(labels, anomaly_scores)\n",
    "            auc_paper = auc(fpr, tpr)\n",
    "            execution_time = time.time() - start_time\n",
    "\n",
    "            # Save run results\n",
    "            run_result = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Run': run + 1,\n",
    "                'AP': ap_score,\n",
    "                'AUC_capymoa': auc_score_capymoa,\n",
    "                'AUC (sklearn)': auc_score,\n",
    "                'AUC (paper)': auc_paper,\n",
    "                'Execution Time (s)': execution_time\n",
    "            }\n",
    "            model_results.append(run_result)\n",
    "            all_results.append(run_result)\n",
    "\n",
    "            # Append metrics for summary\n",
    "            ap_scores.append(ap_score)\n",
    "            auc_scores.append(auc_score)\n",
    "            auc_paper_scores.append(auc_paper)\n",
    "            auc_capymoa_scores.append(auc_score_capymoa)\n",
    "            execution_times.append(execution_time)\n",
    "\n",
    "        # Save checkpoint when using various models and datsets\n",
    "        #results_df = pd.DataFrame(all_results)\n",
    "        #checkpoint_path = os.path.join(results_dir, \"all_run_results_checkpoint.csv\")\n",
    "        #results_df.to_csv(checkpoint_path, index=False)\n",
    "        #print(f\"Checkpoint saved for model {model_name} at {checkpoint_path}\")\n",
    "        # Save checkpoint for only one endpoint\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        checkpoint_path = os.path.join(results_dir, \"streamrhf_hyper100k_results_checkpoint.csv\")\n",
    "        results_df.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"Checkpoint saved for model {model_name} at {checkpoint_path}\")\n",
    "\n",
    "        # Summarize results\n",
    "        metrics = {\n",
    "            'AP': ap_scores,\n",
    "            'AUC (sklearn)': auc_scores,\n",
    "            'AUC (paper)': auc_paper_scores,\n",
    "            'AUC (CapyMOA)': auc_capymoa_scores,\n",
    "            'Execution Time': execution_times\n",
    "        }\n",
    "        summary = {metric: (np.mean(scores), sem(scores) * 1.96) for metric, scores in metrics.items()}\n",
    "        \n",
    "        # Save summary\n",
    "        summary_data = {\n",
    "            'Metric': list(summary.keys()),\n",
    "            'Mean': [mean for mean, _ in summary.values()],\n",
    "            '95% CI': [ci for _, ci in summary.values()]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        model_folder = os.path.join(results_dir, model_name)\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "        summary_path = os.path.join(model_folder, f\"{dataset_name}_summary.csv\")\n",
    "        summary_df.to_csv(summary_path, index=False)\n",
    "        print(f\"Summary saved for {model_name} at {summary_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
