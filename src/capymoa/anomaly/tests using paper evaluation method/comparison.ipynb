{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from capymoa.stream import stream_from_file\n",
    "from capymoa.anomaly import HalfSpaceTrees, OnlineIsolationForest, Autoencoder, StreamRHF\n",
    "from capymoa.evaluation import AnomalyDetectionEvaluator\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, roc_curve, auc\n",
    "from scipy.stats import sem\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset path: /home/infres/cchavez-23/public\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define datasets, their corresponding run counts, and models\n",
    "datasets = {\n",
    "    #\"abalone\": 5,\n",
    "    #\"annthyroid\": 5,\n",
    "    #\"magicgamma\": 1,\n",
    "    \"kdd_ftp\": 5,\n",
    "    #\"mammography\": 1,\n",
    "    #\"thyroid\": 1,\n",
    "    #\"mnist\": 1,\n",
    "    #\"musk\": 1,\n",
    "    #\"satellite\": 1,\n",
    "    #\"satimages\": 1,\n",
    "    #\"spambase\": 1,\n",
    "    #\"shuttle_odds\": 1\n",
    "}\n",
    "\n",
    "models = {\n",
    "    #\"HalfSpaceTrees\": HalfSpaceTrees,\n",
    "    #\"Autoencoder\": Autoencoder,\n",
    "    #\"OnlineIsolationForest\": OnlineIsolationForest,\n",
    "    \"StreamRHF\": StreamRHF\n",
    "}\n",
    "\n",
    "# Define dataset path\n",
    "#local path Cristian\n",
    "#dataset_path = r\"C:\\Users\\aleja\\OneDrive - Universidad Nacional de Colombia\\Documentos\\Institut Polytechnique de Paris\\courses\\P1\\Data Streaming\\project\\actual code\\datasets\\forStefan\\data\\public\"\n",
    "\n",
    "#remote server path Cristian\n",
    "# In Jupyter Notebook or interactive environments, use the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Navigate to the public folder relative to the current working directory\n",
    "dataset_path = os.path.abspath(os.path.join(current_dir, '../../../../../public'))\n",
    "\n",
    "print(\"Dataset path:\", dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: kdd_ftp (Runs: 5)\n",
      "window size 52 for dataset kdd_ftp\n",
      "Running model: StreamRHF\n",
      "our StreamRHF initialized\n",
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/cchavez-23/Stream-Random-Histogram-Forest/src/capymoa/stream/_stream.py:38: UserWarning: target variable includes 2 (< 20) unique values, inferred as categorical, set target_type = 'numeric' if you intend numeric targets\n",
      "  warnings.warn(f'target variable includes {num_unique} (< 20) unique values, inferred as categorical, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: AP = 0.1695, AUC = 0.1491, Time = 382.67s\n",
      "our StreamRHF initialized\n",
      "52\n",
      "Run 2: AP = 0.1695, AUC = 0.1416, Time = 377.93s\n",
      "our StreamRHF initialized\n",
      "52\n",
      "Run 3: AP = 0.1690, AUC = 0.1533, Time = 364.39s\n",
      "our StreamRHF initialized\n",
      "52\n",
      "Run 4: AP = 0.1685, AUC = 0.1466, Time = 377.23s\n",
      "our StreamRHF initialized\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "# Results storage\n",
    "all_results = []\n",
    "\n",
    "# Main loop\n",
    "for dataset_name, n_runs in datasets.items():\n",
    "    print(f\"Dataset: {dataset_name} (Runs: {n_runs})\")\n",
    "\n",
    "    input_path = os.path.join(dataset_path, f\"{dataset_name}.gz\")\n",
    "    output_path = os.path.join(dataset_path, f\"{dataset_name}.csv\")\n",
    "\n",
    "    # Unzip the dataset if needed\n",
    "    if not os.path .exists(output_path):\n",
    "        with gzip.open(input_path, 'rt') as gz_file:\n",
    "            df = pd.read_csv(gz_file)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"CSV saved to: {output_path}\")\n",
    "    with gzip.open(input_path, 'rt') as gz_file:\n",
    "        df = pd.read_csv(gz_file)\n",
    "    window_size = len(df) // 100  # 1% of the data stream\n",
    "    print('window size ' + str(window_size) + ' for dataset ' + str(dataset_name))\n",
    "\n",
    "    stream = stream_from_file(output_path, dataset_name=dataset_name)\n",
    "    schema = stream.get_schema()\n",
    "\n",
    "    # Load labels for metrics\n",
    "    df = pd.read_csv(output_path)\n",
    "    labels = df['label'].to_numpy(dtype='float32')\n",
    "\n",
    "    for model_name, ModelClass in models.items():\n",
    "        print(f\"Running model: {model_name}\")\n",
    "        ap_scores = []\n",
    "        auc_scores = []\n",
    "        auc_paper_scores = []\n",
    "        auc_capymoa_scores = []\n",
    "        execution_times = []\n",
    "        model_results = []\n",
    "\n",
    "        for run in range(n_runs):\n",
    "            learner = ModelClass(schema, window_size=window_size)\n",
    "            evaluator = AnomalyDetectionEvaluator(schema)\n",
    "\n",
    "            stream.restart()  # Restart stream for each run\n",
    "            anomaly_scores = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            while stream.has_more_instances():\n",
    "                instance = stream.next_instance()\n",
    "                proba = learner.score_instance(instance)\n",
    "                #We do 1-proba because for capyMOA models 1 means normal and 0 means anomaly, inverse as in streamrhf\n",
    "                anomaly_scores.append(1-proba)\n",
    "                #anomaly_scores.append(proba)\n",
    "                evaluator.update(instance.y_index, proba)\n",
    "                learner.train(instance)\n",
    "\n",
    "            # Get AUC from evaluator\n",
    "            auc_score_capymoa = evaluator.auc()\n",
    "\n",
    "            #####################################\n",
    "            anomaly_scores = np.array(anomaly_scores)\n",
    "            ap_score = average_precision_score(labels, anomaly_scores)\n",
    "            auc_score = roc_auc_score(labels, anomaly_scores)\n",
    "            fpr, tpr, thresholds = roc_curve(labels, anomaly_scores)\n",
    "            auc_paper = auc(fpr, tpr)\n",
    "            #####################################\n",
    "\n",
    "            execution_time = time.time() - start_time\n",
    "\n",
    "            print(f\"Run {run + 1}: AP = {ap_score:.4f}, AUC = {auc_score:.4f}, Time = {execution_time:.2f}s\")\n",
    "\n",
    "            # Save run results\n",
    "            run_result = {\n",
    "                'Dataset': dataset_name,\n",
    "                'Model': model_name,\n",
    "                'Run': run + 1,\n",
    "                'AP': ap_score,\n",
    "                'AUC_capymoa': auc_score_capymoa,\n",
    "                'AUC (sklearn)': auc_score,\n",
    "                'AUC (paper)': auc_paper,\n",
    "                'Execution Time (s)': execution_time\n",
    "            }\n",
    "            model_results.append(run_result)\n",
    "            all_results.append(run_result)\n",
    "\n",
    "        # Save checkpoint after each model\n",
    "        results_df = pd.DataFrame(all_results)\n",
    "        results_df.to_csv(\"all_run_results_checkpoint.csv\", index=False)\n",
    "        print(f\"Checkpoint saved for model {model_name}\")\n",
    "\n",
    "        # Summarize results for the model\n",
    "        ap_scores = np.array([res['AP'] for res in model_results])\n",
    "        auc_scores = np.array([res['AUC (sklearn)'] for res in model_results])\n",
    "        auc_paper_scores = np.array([res['AUC (paper)'] for res in model_results])\n",
    "        auc_capymoa_scores = np.array([res['AUC_capymoa'] for res in model_results])\n",
    "        execution_times = np.array([res['Execution Time (s)'] for res in model_results])\n",
    "\n",
    "        mean_ap = np.mean(ap_scores)\n",
    "        mean_auc = np.mean(auc_scores)\n",
    "        mean_auc_paper = np.mean(auc_paper_scores)\n",
    "        mean_auc_capymoa = np.mean(auc_capymoa_scores)\n",
    "        mean_time = np.mean(execution_times)\n",
    "        ap_sem = sem(ap_scores)\n",
    "        auc_sem = sem(auc_scores)\n",
    "        auc_paper_sem = sem(auc_paper_scores)\n",
    "        auc_capymoa_sem = sem(auc_capymoa_scores)\n",
    "        time_sem = sem(execution_times)\n",
    "        confidence_level = 1.96\n",
    "        ap_ci = confidence_level * ap_sem\n",
    "        auc_ci = confidence_level * auc_sem\n",
    "        auc_paper_ci = confidence_level * auc_paper_sem\n",
    "        auc_capymoa_ci = confidence_level * auc_capymoa_sem\n",
    "        time_ci = confidence_level * time_sem\n",
    "\n",
    "        print(f\"Summary for {model_name}:\")\n",
    "        print(f\"AP: {mean_ap:.4f} ± {ap_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (sklearn): {mean_auc:.4f} ± {auc_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (paper): {mean_auc_paper:.4f} ± {auc_paper_ci:.4f} (95% CI)\")\n",
    "        print(f\"AUC (CapyMOA): {mean_auc_capymoa:.4f} ± {auc_capymoa_ci:.4f} (95% CI)\")\n",
    "        print(f\"Time: {mean_time:.2f} ± {time_ci:.2f} seconds (95% CI)\")\n",
    "\n",
    "        # Save summary\n",
    "        summary = {\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Metric': ['AP', 'AUC (sklearn)', 'AUC (paper)', 'AUC (CapyMOA)', 'Execution Time'],\n",
    "            'Mean': [mean_ap, mean_auc, mean_auc_paper, mean_auc_capymoa, mean_time],\n",
    "            'CI (95%)': [ap_ci, auc_ci, auc_paper_ci, auc_capymoa_ci, time_ci]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary)\n",
    "\n",
    "        # Create a folder for each model in the current working directory if it doesn't exist\n",
    "        model_folder = os.path.join(os.getcwd(), model_name)\n",
    "        os.makedirs(model_folder, exist_ok=True)\n",
    "        \n",
    "        # Save the summary in the respective model's folder\n",
    "        summary_df.to_csv(os.path.join(model_folder, f\"{dataset_name}_summary.csv\"), index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
